{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-11-30T21:00:47.787193Z",
     "iopub.status.busy": "2022-11-30T21:00:47.786200Z",
     "iopub.status.idle": "2022-11-30T21:00:49.713830Z",
     "shell.execute_reply": "2022-11-30T21:00:49.711988Z",
     "shell.execute_reply.started": "2022-11-30T21:00:47.787086Z"
    }
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/nlp-a2/cuad-main /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T21:00:49.716779Z",
     "iopub.status.busy": "2022-11-30T21:00:49.716344Z",
     "iopub.status.idle": "2022-11-30T21:01:03.562533Z",
     "shell.execute_reply": "2022-11-30T21:01:03.561312Z",
     "shell.execute_reply.started": "2022-11-30T21:00:49.716736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m919.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.20.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.64.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.11.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.12.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.21.6)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.0.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.7.3)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (3.7)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.1.97)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.10.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.7.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.13.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.11.10)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->sentence-transformers) (9.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.0)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=7739fcf762cf2c7d806c2456a51115dc1cbf67a65bf8f7013dbdb8c777892588\n",
      "  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-2.2.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T21:12:17.245538Z",
     "iopub.status.busy": "2022-11-30T21:12:17.244387Z",
     "iopub.status.idle": "2022-11-30T21:12:17.673456Z",
     "shell.execute_reply": "2022-11-30T21:12:17.672421Z",
     "shell.execute_reply.started": "2022-11-30T21:12:17.245487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"/kaggle/working/cuad-main/data/train_separate_questions.json\") as fp:\n",
    "    data = json.load(fp)\n",
    "cnt = 0\n",
    "t_data = []\n",
    "for x in data[\"data\"]:\n",
    "    con_len = len(x[\"paragraphs\"][0][\"context\"].split()) \n",
    "    if con_len < 1024 :\n",
    "        cnt += 1\n",
    "        if cnt == 16:\n",
    "            break\n",
    "        t_data.append(x)\n",
    "print(cnt)\n",
    "data[\"data\"] = t_data\n",
    "data = json.dumps(data)\n",
    "with open(\"/kaggle/working/cuad-main/data/train.json\",\"w\") as fp:\n",
    "    fp.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T21:12:50.017302Z",
     "iopub.status.busy": "2022-11-30T21:12:50.016327Z",
     "iopub.status.idle": "2022-11-30T21:12:50.075231Z",
     "shell.execute_reply": "2022-11-30T21:12:50.074191Z",
     "shell.execute_reply.started": "2022-11-30T21:12:50.017265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "with open(\"/kaggle/working/cuad-main/data/test.json\") as fp:\n",
    "    data = json.load(fp)\n",
    "cnt = 0\n",
    "t_data = []\n",
    "for x in data[\"data\"]:\n",
    "    con_len = len(x[\"paragraphs\"][0][\"context\"].split())\n",
    "    if  con_len < 1024:\n",
    "        cnt+=1\n",
    "        if cnt == 4:\n",
    "            break\n",
    "        t_data.append(x)\n",
    "print(cnt)\n",
    "data[\"data\"] = t_data\n",
    "data = json.dumps(data)\n",
    "with open(\"/kaggle/working/cuad-main/data/test_final.json\",\"w\") as fp:\n",
    "    fp.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T21:12:53.658824Z",
     "iopub.status.busy": "2022-11-30T21:12:53.658228Z",
     "iopub.status.idle": "2022-11-30T21:12:53.664016Z",
     "shell.execute_reply": "2022-11-30T21:12:53.662962Z",
     "shell.execute_reply.started": "2022-11-30T21:12:53.658778Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/kaggle/working/cuad-main/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T21:15:32.582405Z",
     "iopub.status.busy": "2022-11-30T21:15:32.582029Z",
     "iopub.status.idle": "2022-11-30T21:16:58.647761Z",
     "shell.execute_reply": "2022-11-30T21:16:58.646543Z",
     "shell.execute_reply.started": "2022-11-30T21:15:32.582352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:659] 2022-11-30 21:15:36,834 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:708] 2022-11-30 21:15:36,835 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:659] 2022-11-30 21:15:37,505 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:708] 2022-11-30 21:15:37,506 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1781] 2022-11-30 21:15:38,839 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "[INFO|tokenization_utils_base.py:1781] 2022-11-30 21:15:38,839 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1781] 2022-11-30 21:15:38,839 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1781] 2022-11-30 21:15:38,839 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "[INFO|configuration_utils.py:659] 2022-11-30 21:15:39,172 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:708] 2022-11-30 21:15:39,172 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2107] 2022-11-30 21:15:40,464 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "[WARNING|modeling_utils.py:2474] 2022-11-30 21:15:42,051 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2486] 2022-11-30 21:15:42,051 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "balanced_subset_cached_train_bert-base-uncased_512\n",
      "100%|███████████████████████████████████████████| 15/15 [00:02<00:00,  6.95it/s]\n",
      "convert squad examples to features: 100%|█████| 662/662 [00:34<00:00, 19.11it/s]\n",
      "add example index and unique id: 100%|████| 662/662 [00:00<00:00, 340189.81it/s]\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "Epoch:   0%|                                              | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                         | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   6%|██                               | 1/16 [00:01<00:27,  1.84s/it]\u001b[A\n",
      "Iteration:  12%|████▏                            | 2/16 [00:02<00:17,  1.24s/it]\u001b[A\n",
      "Iteration:  19%|██████▏                          | 3/16 [00:03<00:13,  1.05s/it]\u001b[A\n",
      "Iteration:  25%|████████▎                        | 4/16 [00:04<00:11,  1.04it/s]\u001b[A\n",
      "Iteration:  31%|██████████▎                      | 5/16 [00:05<00:09,  1.10it/s]\u001b[A\n",
      "Iteration:  38%|████████████▍                    | 6/16 [00:05<00:08,  1.14it/s]\u001b[A\n",
      "Iteration:  44%|██████████████▍                  | 7/16 [00:06<00:07,  1.16it/s]\u001b[A\n",
      "Iteration:  50%|████████████████▌                | 8/16 [00:07<00:06,  1.18it/s]\u001b[A\n",
      "Iteration:  56%|██████████████████▌              | 9/16 [00:08<00:05,  1.19it/s]\u001b[A\n",
      "Iteration:  62%|████████████████████            | 10/16 [00:09<00:04,  1.20it/s]\u001b[A\n",
      "Iteration:  69%|██████████████████████          | 11/16 [00:10<00:04,  1.21it/s]\u001b[A\n",
      "Iteration:  75%|████████████████████████        | 12/16 [00:10<00:03,  1.21it/s]\u001b[A\n",
      "Iteration:  81%|██████████████████████████      | 13/16 [00:11<00:02,  1.21it/s]\u001b[A\n",
      "Iteration:  88%|████████████████████████████    | 14/16 [00:12<00:01,  1.21it/s]\u001b[A\n",
      "Iteration:  94%|██████████████████████████████  | 15/16 [00:13<00:00,  1.22it/s]\u001b[A\n",
      "Iteration: 100%|████████████████████████████████| 16/16 [00:13<00:00,  1.14it/s]\u001b[A\n",
      "Epoch: 100%|██████████████████████████████████████| 1/1 [00:13<00:00, 13.99s/it]\n",
      "[INFO|configuration_utils.py:446] 2022-11-30 21:16:35,659 >> Configuration saved in /kaggle/working/cuad-main/train_models/bert-base-uncased/config.json\n",
      "[INFO|modeling_utils.py:1660] 2022-11-30 21:16:36,432 >> Model weights saved in /kaggle/working/cuad-main/train_models/bert-base-uncased/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2123] 2022-11-30 21:16:36,433 >> tokenizer config file saved in /kaggle/working/cuad-main/train_models/bert-base-uncased/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2130] 2022-11-30 21:16:36,433 >> Special tokens file saved in /kaggle/working/cuad-main/train_models/bert-base-uncased/special_tokens_map.json\n",
      "[INFO|configuration_utils.py:657] 2022-11-30 21:16:36,454 >> loading configuration file /kaggle/working/cuad-main/train_models/bert-base-uncased/config.json\n",
      "[INFO|configuration_utils.py:708] 2022-11-30 21:16:36,455 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"/kaggle/working/cuad-main/train_models/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2105] 2022-11-30 21:16:36,455 >> loading weights file /kaggle/working/cuad-main/train_models/bert-base-uncased/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:2483] 2022-11-30 21:16:37,889 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:2492] 2022-11-30 21:16:37,889 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /kaggle/working/cuad-main/train_models/bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "[INFO|tokenization_utils_base.py:1701] 2022-11-30 21:16:37,891 >> Didn't find file /kaggle/working/cuad-main/train_models/bert-base-uncased/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1779] 2022-11-30 21:16:37,891 >> loading file /kaggle/working/cuad-main/train_models/bert-base-uncased/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1779] 2022-11-30 21:16:37,891 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1779] 2022-11-30 21:16:37,891 >> loading file /kaggle/working/cuad-main/train_models/bert-base-uncased/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1779] 2022-11-30 21:16:37,892 >> loading file /kaggle/working/cuad-main/train_models/bert-base-uncased/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:657] 2022-11-30 21:16:38,073 >> loading configuration file /kaggle/working/cuad-main/train_models/bert-base-uncased/config.json\n",
      "[INFO|configuration_utils.py:708] 2022-11-30 21:16:38,074 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"/kaggle/working/cuad-main/train_models/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2105] 2022-11-30 21:16:38,074 >> loading weights file /kaggle/working/cuad-main/train_models/bert-base-uncased/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:2483] 2022-11-30 21:16:39,451 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:2492] 2022-11-30 21:16:39,452 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /kaggle/working/cuad-main/train_models/bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "balanced_subset_cached_dev_bert-base-uncased_512\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  8.14it/s]\n",
      "convert squad examples to features: 100%|█████| 123/123 [00:04<00:00, 26.31it/s]\n",
      "add example index and unique id: 100%|████| 123/123 [00:00<00:00, 321432.64it/s]\n",
      "Evaluating: 100%|███████████████████████████████| 19/19 [00:04<00:00,  3.92it/s]\n",
      "OrderedDict([('exact', 88.6178861788618), ('f1', 88.6178861788618), ('total', 123), ('HasAns_exact', 0.0), ('HasAns_f1', 0.0), ('HasAns_total', 14), ('NoAns_exact', 100.0), ('NoAns_f1', 100.0), ('NoAns_total', 109), ('best_exact', 88.6178861788618), ('best_exact_thresh', 0.0), ('best_f1', 88.6178861788618), ('best_f1_thresh', 0.0)])\n"
     ]
    }
   ],
   "source": [
    "!python train.py --output_dir /kaggle/working/cuad-main/train_models/bert-base-uncased --model_type bert-base-uncased         --model_name_or_path bert-base-uncased --train_file /kaggle/working/cuad-main/data/train.json --predict_file /kaggle/working/cuad-main/data/test_final.json         --do_train         --do_eval         --version_2_with_negative         --learning_rate 1e-4         --num_train_epochs 1        --per_gpu_eval_batch_size=16         --per_gpu_train_batch_size=16         --max_seq_length 512         --max_answer_length 512         --doc_stride 256         --save_steps 1000         --n_best_size 20         --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T21:17:09.459733Z",
     "iopub.status.busy": "2022-11-30T21:17:09.459286Z",
     "iopub.status.idle": "2022-11-30T21:17:09.498239Z",
     "shell.execute_reply": "2022-11-30T21:17:09.497306Z",
     "shell.execute_reply.started": "2022-11-30T21:17:09.459690Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "IOU_THRESH = 0.5\n",
    "\n",
    "def get_questions_from_csv():\n",
    "    df = pd.read_csv(\"category_descriptions.csv\")\n",
    "    q_dict = {}\n",
    "    for i in range(df.shape[0]):\n",
    "        category = df.iloc[i, 0].split(\"Category: \")[1]\n",
    "        description = df.iloc[i, 1].split(\"Description: \")[1]\n",
    "        q_dict[category.title()] = description\n",
    "    return q_dict\n",
    "\n",
    "qtype_dict = get_questions_from_csv()\n",
    "\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        dict = json.load(f)\n",
    "    return dict\n",
    "\n",
    "\n",
    "def get_preds(nbest_preds_dict, conf=None):\n",
    "    results = {}\n",
    "    for question_id in nbest_preds_dict:\n",
    "        list_of_pred_dicts = nbest_preds_dict[question_id]\n",
    "        preds = {}\n",
    "        for pred_dict in list_of_pred_dicts:\n",
    "            text = pred_dict[\"text\"]\n",
    "            prob = pred_dict[\"probability\"]\n",
    "            if not text == \"\":  # don't count empty string as a prediction\n",
    "                preds[text] = prob\n",
    "        preds_list = [pred for pred in preds.keys() if preds[pred] > conf]\n",
    "        results[question_id] = preds_list\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_answers(test_json_dict):\n",
    "    results = {}\n",
    "\n",
    "    data = test_json_dict[\"data\"]\n",
    "    for contract in data:\n",
    "        for para in contract[\"paragraphs\"]:\n",
    "            qas = para[\"qas\"]\n",
    "            for qa in qas:\n",
    "                id = qa[\"id\"]\n",
    "                answers = qa[\"answers\"]\n",
    "                answers = [answers[i][\"text\"] for i in range(len(answers))]\n",
    "                results[id] = answers\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_jaccard(gt, pred):\n",
    "    remove_tokens = [\".\", \",\", \";\", \":\"]\n",
    "    for token in remove_tokens:\n",
    "        gt = gt.replace(token, \"\")\n",
    "        pred = pred.replace(token, \"\")\n",
    "    gt = gt.lower()\n",
    "    pred = pred.lower()\n",
    "    gt = gt.replace(\"/\", \" \")\n",
    "    pred = pred.replace(\"/\", \" \")\n",
    "\n",
    "    gt_words = set(gt.split(\" \"))\n",
    "    pred_words = set(pred.split(\" \"))\n",
    "\n",
    "    intersection = gt_words.intersection(pred_words)\n",
    "    union = gt_words.union(pred_words)\n",
    "    jaccard = len(intersection) / len(union)\n",
    "    return jaccard\n",
    "\n",
    "\n",
    "def compute_precision_recall(gt_dict, preds_dict, category=None):\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "\n",
    "    for key in gt_dict:\n",
    "        if category and category not in key:\n",
    "            continue\n",
    "\n",
    "        substr_ok = \"Parties\" in key\n",
    "\n",
    "        answers = gt_dict[key]\n",
    "        preds = preds_dict[key]\n",
    "\n",
    "        # first check if answers is empty\n",
    "        if len(answers) == 0:\n",
    "            if len(preds) > 0:\n",
    "                fp += len(preds)  # false positive for each one\n",
    "        else:\n",
    "            for ans in answers:\n",
    "                assert len(ans) > 0\n",
    "                # check if there is a match\n",
    "                match_found = False\n",
    "                for pred in preds:\n",
    "                    if substr_ok:\n",
    "                        is_match = get_jaccard(ans, pred) >= IOU_THRESH or ans in pred\n",
    "                    else:\n",
    "                        is_match = get_jaccard(ans, pred) >= IOU_THRESH\n",
    "                    if is_match:\n",
    "                        match_found = True\n",
    "\n",
    "                if match_found:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fn += 1\n",
    "\n",
    "            # now also get any fps by looping through preds\n",
    "            for pred in preds:\n",
    "                # Check if there's a match. if so, don't count (don't want to double count based on the above)\n",
    "                # but if there's no match, then this is a false positive.\n",
    "                # (Note: we get the true positives in the above loop instead of this loop so that we don't double count\n",
    "                # multiple predictions that are matched with the same answer.)\n",
    "                match_found = False\n",
    "                for ans in answers:\n",
    "                    assert len(ans) > 0\n",
    "                    if substr_ok:\n",
    "                        is_match = get_jaccard(ans, pred) >= IOU_THRESH or ans in pred\n",
    "                    else:\n",
    "                        is_match = get_jaccard(ans, pred) >= IOU_THRESH\n",
    "                    if is_match:\n",
    "                        match_found = True\n",
    "\n",
    "                if not match_found:\n",
    "                    fp += 1\n",
    "\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else np.nan\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else np.nan\n",
    "\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def process_precisions(precision):\n",
    "    \"\"\"\n",
    "    Processes precisions to ensure that precision and recall don't both get worse\n",
    "    Assumes the list precision is sorted in order of recalls\n",
    "    \"\"\"\n",
    "    precision_best = precision[::-1]\n",
    "    for i in range(1, len(precision_best)):\n",
    "        precision_best[i] = max(precision_best[i-1], precision_best[i])\n",
    "    precision = precision_best[::-1]\n",
    "    return precision\n",
    "\n",
    "\n",
    "def get_prec_at_recall(precisions, recalls, confs, recall_thresh=0.9):\n",
    "    \"\"\"\n",
    "    Assumes recalls are sorted in increasing order\n",
    "    \"\"\"\n",
    "    processed_precisions = process_precisions(precisions)\n",
    "    prec_at_recall = 0\n",
    "    for prec, recall, conf in zip(processed_precisions, recalls, confs):\n",
    "        if recall >= recall_thresh:\n",
    "            prec_at_recall = prec\n",
    "            break\n",
    "    return prec_at_recall, conf\n",
    "\n",
    "\n",
    "def get_precisions_recalls(pred_dict, gt_dict, category=None):\n",
    "    precisions = [1]\n",
    "    recalls = [0]\n",
    "    confs = []\n",
    "    for conf in list(np.arange(0.99, 0, -0.01)) + [0.001, 0]:\n",
    "        conf_thresh_pred_dict = get_preds(pred_dict, conf)\n",
    "        prec, recall = compute_precision_recall(gt_dict, conf_thresh_pred_dict, category=category)\n",
    "        precisions.append(prec)\n",
    "        recalls.append(recall)\n",
    "        confs.append(conf)\n",
    "    return precisions, recalls, confs\n",
    "\n",
    "\n",
    "def get_aupr(precisions, recalls):\n",
    "    processed_precisions = process_precisions(precisions)\n",
    "    aupr = metrics.auc(recalls, processed_precisions)\n",
    "    if np.isnan(aupr):\n",
    "        return 0\n",
    "    return aupr\n",
    "\n",
    "\n",
    "def get_results(model_path, gt_dict, verbose=False):\n",
    "    predictions_path = os.path.join(model_path, \"nbest_predictions_.json\")\n",
    "    name = model_path.split(\"/\")[-1]\n",
    "\n",
    "    pred_dict = load_json(predictions_path)\n",
    "    print(len(list(gt_dict.keys())))\n",
    "    assert sorted(list(pred_dict.keys())) == sorted(list(gt_dict.keys()))\n",
    "\n",
    "    precisions, recalls, confs = get_precisions_recalls(pred_dict, gt_dict)\n",
    "    prec_at_90_recall, _ = get_prec_at_recall(precisions, recalls, confs, recall_thresh=0.9)\n",
    "    prec_at_80_recall, _ = get_prec_at_recall(precisions, recalls, confs, recall_thresh=0.8)\n",
    "    aupr = get_aupr(precisions, recalls)\n",
    "\n",
    "    \n",
    "    print(\"AUPR: {:.3f}, Precision at 80% Recall: {:.3f}, Precision at 90% Recall: {:.3f}\".format(aupr, prec_at_80_recall, prec_at_90_recall))\n",
    "\n",
    "    # now save results as a dataframe and return\n",
    "    results = {\"name\": name, \"aupr\": aupr, \"prec_at_80_recall\": prec_at_80_recall, \"prec_at_90_recall\": prec_at_90_recall}\n",
    "    return results\n",
    "\n",
    "\n",
    "# if name == \"main\":\n",
    "#     test_json_path = \"./data/test.json\"\n",
    "#     model_path = \"./trained_models/roberta-base\"\n",
    "#     save_dir = \"./results\"\n",
    "#     if not os.path.exists(save_dir): os.mkdir(save_dir)\n",
    "\n",
    "#     gt_dict = load_json(test_json_path)\n",
    "#     gt_dict = get_answers(gt_dict)\n",
    "\n",
    "#     results = get_results(model_path, gt_dict, verbose=True)\n",
    "\n",
    "#     save_path = os.path.join(save_dir, \"{}.json\".format(model_path.split(\"/\")[-1]))\n",
    "#     with open(save_path, \"w\") as f:\n",
    "#         f.write(\"{}\\n\".format(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T21:17:15.719907Z",
     "iopub.status.busy": "2022-11-30T21:17:15.719520Z",
     "iopub.status.idle": "2022-11-30T21:17:15.902552Z",
     "shell.execute_reply": "2022-11-30T21:17:15.901455Z",
     "shell.execute_reply.started": "2022-11-30T21:17:15.719874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "AUPR: 0.003, Precision at 80% Recall: 0.000, Precision at 90% Recall: 0.000\n"
     ]
    }
   ],
   "source": [
    "gt_dict = load_json(\"/kaggle/working/cuad-main/data/test_final.json\")\n",
    "gt_dict = get_answers(gt_dict)\n",
    "len(gt_dict.keys())\n",
    "\n",
    "results = get_results(\"./train_models/bert-base-uncased/\", gt_dict, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
