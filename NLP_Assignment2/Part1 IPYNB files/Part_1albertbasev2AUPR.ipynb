{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!cp -r /kaggle/input/nlp-a2/cuad-main /kaggle/working/","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-01T02:11:10.505293Z","iopub.execute_input":"2022-12-01T02:11:10.505745Z","iopub.status.idle":"2022-12-01T02:11:13.264961Z","shell.execute_reply.started":"2022-12-01T02:11:10.505635Z","shell.execute_reply":"2022-12-01T02:11:13.263227Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install -U sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2022-12-01T02:11:13.268369Z","iopub.execute_input":"2022-12-01T02:11:13.269256Z","iopub.status.idle":"2022-12-01T02:11:28.825514Z","shell.execute_reply.started":"2022-12-01T02:11:13.269207Z","shell.execute_reply":"2022-12-01T02:11:28.824235Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m286.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.20.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.64.0)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.11.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.12.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.21.6)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.0.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.7.3)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (3.7)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.1.97)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.10.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.7.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.13.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.11.10)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers) (1.0.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers) (8.0.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->sentence-transformers) (9.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.9.24)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.0)\nBuilding wheels for collected packages: sentence-transformers\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=432f42718e9ae338706d30002b79b7cd27d53867c631e679094ae8b8b33b6fbc\n  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\nSuccessfully built sentence-transformers\nInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-2.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import json\nwith open(\"/kaggle/working/cuad-main/data/train_separate_questions.json\") as fp:\n    data = json.load(fp)\ncnt = 0\nt_data = []\nfor x in data[\"data\"]:\n    con_len = len(x[\"paragraphs\"][0][\"context\"].split()) \n    if con_len < 1024 :\n        cnt += 1\n        if cnt == 16:\n            break\n        t_data.append(x)\nprint(cnt)\ndata[\"data\"] = t_data\ndata = json.dumps(data)\nwith open(\"/kaggle/working/cuad-main/data/train.json\",\"w\") as fp:\n    fp.write(data)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T02:11:28.830656Z","iopub.execute_input":"2022-12-01T02:11:28.833469Z","iopub.status.idle":"2022-12-01T02:11:29.410300Z","shell.execute_reply.started":"2022-12-01T02:11:28.833423Z","shell.execute_reply":"2022-12-01T02:11:29.409066Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"16\n","output_type":"stream"}]},{"cell_type":"code","source":"with open(\"/kaggle/working/cuad-main/data/test.json\") as fp:\n    data = json.load(fp)\ncnt = 0\nt_data = []\nfor x in data[\"data\"]:\n    con_len = len(x[\"paragraphs\"][0][\"context\"].split())\n    if  con_len < 1024:\n        cnt+=1\n        if cnt == 4:\n            break\n        t_data.append(x)\nprint(cnt)\ndata[\"data\"] = t_data\ndata = json.dumps(data)\nwith open(\"/kaggle/working/cuad-main/data/test_final.json\",\"w\") as fp:\n    fp.write(data)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T02:11:29.413097Z","iopub.execute_input":"2022-12-01T02:11:29.413615Z","iopub.status.idle":"2022-12-01T02:11:29.476122Z","shell.execute_reply.started":"2022-12-01T02:11:29.413574Z","shell.execute_reply":"2022-12-01T02:11:29.475002Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"4\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.chdir(\"/kaggle/working/cuad-main/\")","metadata":{"execution":{"iopub.status.busy":"2022-12-01T02:11:29.477900Z","iopub.execute_input":"2022-12-01T02:11:29.478331Z","iopub.status.idle":"2022-12-01T02:11:29.483613Z","shell.execute_reply.started":"2022-12-01T02:11:29.478292Z","shell.execute_reply":"2022-12-01T02:11:29.482249Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!python train.py --output_dir /kaggle/working/cuad-main/train_models/albert-base-v2 --model_type albert-base-v2         --model_name_or_path albert-base-v2 --train_file /kaggle/working/cuad-main/data/train.json --predict_file /kaggle/working/cuad-main/data/test_final.json         --do_train         --do_eval         --version_2_with_negative         --learning_rate 1e-4         --num_train_epochs 1        --per_gpu_eval_batch_size=16         --per_gpu_train_batch_size=16         --max_seq_length 512         --max_answer_length 512         --doc_stride 256         --save_steps 1000         --n_best_size 20         --overwrite_output_dir","metadata":{"execution":{"iopub.status.busy":"2022-12-01T02:11:29.485209Z","iopub.execute_input":"2022-12-01T02:11:29.485899Z","iopub.status.idle":"2022-12-01T02:13:25.323285Z","shell.execute_reply.started":"2022-12-01T02:11:29.485856Z","shell.execute_reply":"2022-12-01T02:13:25.321853Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[INFO|hub.py:592] 2022-12-01 02:11:38,037 >> https://huggingface.co/albert-base-v2/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpwadgear4\nDownloading: 100%|██████████████████████████████| 684/684 [00:00<00:00, 529kB/s]\n[INFO|hub.py:596] 2022-12-01 02:11:39,052 >> storing https://huggingface.co/albert-base-v2/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n[INFO|hub.py:604] 2022-12-01 02:11:39,052 >> creating metadata file for /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n[INFO|configuration_utils.py:659] 2022-12-01 02:11:39,053 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n[INFO|configuration_utils.py:708] 2022-12-01 02:11:39,054 >> Model config AlbertConfig {\n  \"_name_or_path\": \"albert-base-v2\",\n  \"architectures\": [\n    \"AlbertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0,\n  \"bos_token_id\": 2,\n  \"classifier_dropout_prob\": 0.1,\n  \"down_scale_factor\": 1,\n  \"embedding_size\": 128,\n  \"eos_token_id\": 3,\n  \"gap_size\": 0,\n  \"hidden_act\": \"gelu_new\",\n  \"hidden_dropout_prob\": 0,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"inner_group_num\": 1,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"albert\",\n  \"net_structure_type\": 0,\n  \"num_attention_heads\": 12,\n  \"num_hidden_groups\": 1,\n  \"num_hidden_layers\": 12,\n  \"num_memory_blocks\": 0,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 30000\n}\n\n[INFO|tokenization_auto.py:392] 2022-12-01 02:11:40,068 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n[INFO|configuration_utils.py:659] 2022-12-01 02:11:41,080 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n[INFO|configuration_utils.py:708] 2022-12-01 02:11:41,081 >> Model config AlbertConfig {\n  \"_name_or_path\": \"albert-base-v2\",\n  \"architectures\": [\n    \"AlbertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0,\n  \"bos_token_id\": 2,\n  \"classifier_dropout_prob\": 0.1,\n  \"down_scale_factor\": 1,\n  \"embedding_size\": 128,\n  \"eos_token_id\": 3,\n  \"gap_size\": 0,\n  \"hidden_act\": \"gelu_new\",\n  \"hidden_dropout_prob\": 0,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"inner_group_num\": 1,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"albert\",\n  \"net_structure_type\": 0,\n  \"num_attention_heads\": 12,\n  \"num_hidden_groups\": 1,\n  \"num_hidden_layers\": 12,\n  \"num_memory_blocks\": 0,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 30000\n}\n\n[INFO|hub.py:592] 2022-12-01 02:11:42,093 >> https://huggingface.co/albert-base-v2/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpr0lbfndn\nDownloading: 100%|████████████████████████████| 742k/742k [00:01<00:00, 760kB/s]\n[INFO|hub.py:596] 2022-12-01 02:11:44,140 >> storing https://huggingface.co/albert-base-v2/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n[INFO|hub.py:604] 2022-12-01 02:11:44,140 >> creating metadata file for /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n[INFO|tokenization_utils_base.py:1781] 2022-12-01 02:11:47,193 >> loading file https://huggingface.co/albert-base-v2/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n[INFO|tokenization_utils_base.py:1781] 2022-12-01 02:11:47,193 >> loading file https://huggingface.co/albert-base-v2/resolve/main/added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:1781] 2022-12-01 02:11:47,193 >> loading file https://huggingface.co/albert-base-v2/resolve/main/special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:1781] 2022-12-01 02:11:47,193 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer_config.json from cache at None\n[INFO|configuration_utils.py:659] 2022-12-01 02:11:48,209 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n[INFO|configuration_utils.py:708] 2022-12-01 02:11:48,210 >> Model config AlbertConfig {\n  \"_name_or_path\": \"albert-base-v2\",\n  \"architectures\": [\n    \"AlbertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0,\n  \"bos_token_id\": 2,\n  \"classifier_dropout_prob\": 0.1,\n  \"down_scale_factor\": 1,\n  \"embedding_size\": 128,\n  \"eos_token_id\": 3,\n  \"gap_size\": 0,\n  \"hidden_act\": \"gelu_new\",\n  \"hidden_dropout_prob\": 0,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"inner_group_num\": 1,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"albert\",\n  \"net_structure_type\": 0,\n  \"num_attention_heads\": 12,\n  \"num_hidden_groups\": 1,\n  \"num_hidden_layers\": 12,\n  \"num_memory_blocks\": 0,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 30000\n}\n\n[INFO|hub.py:592] 2022-12-01 02:11:50,454 >> https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpoc299jsc\nDownloading: 100%|█████████████████████████| 45.2M/45.2M [00:05<00:00, 8.41MB/s]\n[INFO|hub.py:596] 2022-12-01 02:11:57,126 >> storing https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n[INFO|hub.py:604] 2022-12-01 02:11:57,126 >> creating metadata file for /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n[INFO|modeling_utils.py:2107] 2022-12-01 02:11:57,127 >> loading weights file https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n[WARNING|modeling_utils.py:2474] 2022-12-01 02:11:57,289 >> Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForQuestionAnswering: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight']\n- This IS expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n[WARNING|modeling_utils.py:2486] 2022-12-01 02:11:57,289 >> Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nbalanced_subset_cached_train_albert-base-v2_512\n100%|███████████████████████████████████████████| 15/15 [00:01<00:00,  7.78it/s]\nconvert squad examples to features: 100%|█████| 662/662 [00:41<00:00, 16.02it/s]\nadd example index and unique id: 100%|████| 662/662 [00:00<00:00, 192660.92it/s]\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\nEpoch:   0%|                                              | 0/1 [00:00<?, ?it/s]\nIteration:   0%|                                         | 0/19 [00:00<?, ?it/s]\u001b[A\nIteration:   5%|█▋                               | 1/19 [00:02<00:37,  2.07s/it]\u001b[A\nIteration:  11%|███▍                             | 2/19 [00:02<00:23,  1.36s/it]\u001b[A\nIteration:  16%|█████▏                           | 3/19 [00:03<00:18,  1.13s/it]\u001b[A\nIteration:  21%|██████▉                          | 4/19 [00:04<00:15,  1.02s/it]\u001b[A\nIteration:  26%|████████▋                        | 5/19 [00:05<00:13,  1.04it/s]\u001b[A\nIteration:  32%|██████████▍                      | 6/19 [00:06<00:12,  1.08it/s]\u001b[A\nIteration:  37%|████████████▏                    | 7/19 [00:07<00:10,  1.10it/s]\u001b[A\nIteration:  42%|█████████████▉                   | 8/19 [00:08<00:09,  1.12it/s]\u001b[A\nIteration:  47%|███████████████▋                 | 9/19 [00:08<00:08,  1.13it/s]\u001b[A\nIteration:  53%|████████████████▊               | 10/19 [00:09<00:07,  1.14it/s]\u001b[A\nIteration:  58%|██████████████████▌             | 11/19 [00:10<00:06,  1.15it/s]\u001b[A\nIteration:  63%|████████████████████▏           | 12/19 [00:11<00:06,  1.15it/s]\u001b[A\nIteration:  68%|█████████████████████▉          | 13/19 [00:12<00:05,  1.16it/s]\u001b[A\nIteration:  74%|███████████████████████▌        | 14/19 [00:13<00:04,  1.16it/s]\u001b[A\nIteration:  79%|█████████████████████████▎      | 15/19 [00:14<00:03,  1.16it/s]\u001b[A\nIteration:  84%|██████████████████████████▉     | 16/19 [00:14<00:02,  1.16it/s]\u001b[A\nIteration:  89%|████████████████████████████▋   | 17/19 [00:15<00:01,  1.16it/s]\u001b[A\nIteration: 100%|████████████████████████████████| 19/19 [00:16<00:00,  1.13it/s]\u001b[A\nEpoch: 100%|██████████████████████████████████████| 1/1 [00:16<00:00, 16.74s/it]\n[INFO|configuration_utils.py:446] 2022-12-01 02:13:02,298 >> Configuration saved in /kaggle/working/cuad-main/train_models/albert-base-v2/config.json\n[INFO|modeling_utils.py:1660] 2022-12-01 02:13:02,385 >> Model weights saved in /kaggle/working/cuad-main/train_models/albert-base-v2/pytorch_model.bin\n[INFO|tokenization_utils_base.py:2123] 2022-12-01 02:13:02,386 >> tokenizer config file saved in /kaggle/working/cuad-main/train_models/albert-base-v2/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2130] 2022-12-01 02:13:02,386 >> Special tokens file saved in /kaggle/working/cuad-main/train_models/albert-base-v2/special_tokens_map.json\n[INFO|configuration_utils.py:657] 2022-12-01 02:13:02,388 >> loading configuration file /kaggle/working/cuad-main/train_models/albert-base-v2/config.json\n[INFO|configuration_utils.py:708] 2022-12-01 02:13:02,389 >> Model config AlbertConfig {\n  \"_name_or_path\": \"/kaggle/working/cuad-main/train_models/albert-base-v2\",\n  \"architectures\": [\n    \"AlbertForQuestionAnswering\"\n  ],\n  \"attention_probs_dropout_prob\": 0,\n  \"bos_token_id\": 2,\n  \"classifier_dropout_prob\": 0.1,\n  \"down_scale_factor\": 1,\n  \"embedding_size\": 128,\n  \"eos_token_id\": 3,\n  \"gap_size\": 0,\n  \"hidden_act\": \"gelu_new\",\n  \"hidden_dropout_prob\": 0,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"inner_group_num\": 1,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"albert\",\n  \"net_structure_type\": 0,\n  \"num_attention_heads\": 12,\n  \"num_hidden_groups\": 1,\n  \"num_hidden_layers\": 12,\n  \"num_memory_blocks\": 0,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 30000\n}\n\n[INFO|modeling_utils.py:2105] 2022-12-01 02:13:02,389 >> loading weights file /kaggle/working/cuad-main/train_models/albert-base-v2/pytorch_model.bin\n[INFO|modeling_utils.py:2483] 2022-12-01 02:13:02,530 >> All model checkpoint weights were used when initializing AlbertForQuestionAnswering.\n\n[INFO|modeling_utils.py:2492] 2022-12-01 02:13:02,530 >> All the weights of AlbertForQuestionAnswering were initialized from the model checkpoint at /kaggle/working/cuad-main/train_models/albert-base-v2.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForQuestionAnswering for predictions without further training.\n[INFO|tokenization_utils_base.py:1701] 2022-12-01 02:13:02,531 >> Didn't find file /kaggle/working/cuad-main/train_models/albert-base-v2/added_tokens.json. We won't load it.\n[INFO|tokenization_utils_base.py:1779] 2022-12-01 02:13:02,531 >> loading file /kaggle/working/cuad-main/train_models/albert-base-v2/spiece.model\n[INFO|tokenization_utils_base.py:1779] 2022-12-01 02:13:02,531 >> loading file None\n[INFO|tokenization_utils_base.py:1779] 2022-12-01 02:13:02,531 >> loading file /kaggle/working/cuad-main/train_models/albert-base-v2/special_tokens_map.json\n[INFO|tokenization_utils_base.py:1779] 2022-12-01 02:13:02,531 >> loading file /kaggle/working/cuad-main/train_models/albert-base-v2/tokenizer_config.json\n[INFO|configuration_utils.py:657] 2022-12-01 02:13:02,601 >> loading configuration file /kaggle/working/cuad-main/train_models/albert-base-v2/config.json\n[INFO|configuration_utils.py:708] 2022-12-01 02:13:02,602 >> Model config AlbertConfig {\n  \"_name_or_path\": \"/kaggle/working/cuad-main/train_models/albert-base-v2\",\n  \"architectures\": [\n    \"AlbertForQuestionAnswering\"\n  ],\n  \"attention_probs_dropout_prob\": 0,\n  \"bos_token_id\": 2,\n  \"classifier_dropout_prob\": 0.1,\n  \"down_scale_factor\": 1,\n  \"embedding_size\": 128,\n  \"eos_token_id\": 3,\n  \"gap_size\": 0,\n  \"hidden_act\": \"gelu_new\",\n  \"hidden_dropout_prob\": 0,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"inner_group_num\": 1,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"albert\",\n  \"net_structure_type\": 0,\n  \"num_attention_heads\": 12,\n  \"num_hidden_groups\": 1,\n  \"num_hidden_layers\": 12,\n  \"num_memory_blocks\": 0,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 30000\n}\n\n[INFO|modeling_utils.py:2105] 2022-12-01 02:13:02,603 >> loading weights file /kaggle/working/cuad-main/train_models/albert-base-v2/pytorch_model.bin\n[INFO|modeling_utils.py:2483] 2022-12-01 02:13:02,736 >> All model checkpoint weights were used when initializing AlbertForQuestionAnswering.\n\n[INFO|modeling_utils.py:2492] 2022-12-01 02:13:02,736 >> All the weights of AlbertForQuestionAnswering were initialized from the model checkpoint at /kaggle/working/cuad-main/train_models/albert-base-v2.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForQuestionAnswering for predictions without further training.\nbalanced_subset_cached_dev_albert-base-v2_512\n100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 12.74it/s]\nconvert squad examples to features: 100%|█████| 123/123 [00:06<00:00, 20.13it/s]\nadd example index and unique id: 100%|████| 123/123 [00:00<00:00, 182232.21it/s]\nEvaluating: 100%|███████████████████████████████| 34/34 [00:10<00:00,  3.38it/s]\nOrderedDict([('exact', 88.6178861788618), ('f1', 88.6178861788618), ('total', 123), ('HasAns_exact', 0.0), ('HasAns_f1', 0.0), ('HasAns_total', 14), ('NoAns_exact', 100.0), ('NoAns_f1', 100.0), ('NoAns_total', 109), ('best_exact', 88.6178861788618), ('best_exact_thresh', 0.0), ('best_f1', 88.6178861788618), ('best_f1_thresh', 0.0)])\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nimport numpy as np\nfrom sklearn import metrics\n\nIOU_THRESH = 0.5\n\ndef get_questions_from_csv():\n    df = pd.read_csv(\"category_descriptions.csv\")\n    q_dict = {}\n    for i in range(df.shape[0]):\n        category = df.iloc[i, 0].split(\"Category: \")[1]\n        description = df.iloc[i, 1].split(\"Description: \")[1]\n        q_dict[category.title()] = description\n    return q_dict\n\nqtype_dict = get_questions_from_csv()\n\n\ndef load_json(path):\n    with open(path, \"r\") as f:\n        dict = json.load(f)\n    return dict\n\n\ndef get_preds(nbest_preds_dict, conf=None):\n    results = {}\n    for question_id in nbest_preds_dict:\n        list_of_pred_dicts = nbest_preds_dict[question_id]\n        preds = {}\n        for pred_dict in list_of_pred_dicts:\n            text = pred_dict[\"text\"]\n            prob = pred_dict[\"probability\"]\n            if not text == \"\":  # don't count empty string as a prediction\n                preds[text] = prob\n        preds_list = [pred for pred in preds.keys() if preds[pred] > conf]\n        results[question_id] = preds_list\n    return results\n\n\ndef get_answers(test_json_dict):\n    results = {}\n\n    data = test_json_dict[\"data\"]\n    for contract in data:\n        for para in contract[\"paragraphs\"]:\n            qas = para[\"qas\"]\n            for qa in qas:\n                id = qa[\"id\"]\n                answers = qa[\"answers\"]\n                answers = [answers[i][\"text\"] for i in range(len(answers))]\n                results[id] = answers\n\n    return results\n\n\ndef get_jaccard(gt, pred):\n    remove_tokens = [\".\", \",\", \";\", \":\"]\n    for token in remove_tokens:\n        gt = gt.replace(token, \"\")\n        pred = pred.replace(token, \"\")\n    gt = gt.lower()\n    pred = pred.lower()\n    gt = gt.replace(\"/\", \" \")\n    pred = pred.replace(\"/\", \" \")\n\n    gt_words = set(gt.split(\" \"))\n    pred_words = set(pred.split(\" \"))\n\n    intersection = gt_words.intersection(pred_words)\n    union = gt_words.union(pred_words)\n    jaccard = len(intersection) / len(union)\n    return jaccard\n\n\ndef compute_precision_recall(gt_dict, preds_dict, category=None):\n    tp, fp, fn = 0, 0, 0\n\n    for key in gt_dict:\n        if category and category not in key:\n            continue\n\n        substr_ok = \"Parties\" in key\n\n        answers = gt_dict[key]\n        preds = preds_dict[key]\n\n        # first check if answers is empty\n        if len(answers) == 0:\n            if len(preds) > 0:\n                fp += len(preds)  # false positive for each one\n        else:\n            for ans in answers:\n                assert len(ans) > 0\n                # check if there is a match\n                match_found = False\n                for pred in preds:\n                    if substr_ok:\n                        is_match = get_jaccard(ans, pred) >= IOU_THRESH or ans in pred\n                    else:\n                        is_match = get_jaccard(ans, pred) >= IOU_THRESH\n                    if is_match:\n                        match_found = True\n\n                if match_found:\n                    tp += 1\n                else:\n                    fn += 1\n\n            # now also get any fps by looping through preds\n            for pred in preds:\n                # Check if there's a match. if so, don't count (don't want to double count based on the above)\n                # but if there's no match, then this is a false positive.\n                # (Note: we get the true positives in the above loop instead of this loop so that we don't double count\n                # multiple predictions that are matched with the same answer.)\n                match_found = False\n                for ans in answers:\n                    assert len(ans) > 0\n                    if substr_ok:\n                        is_match = get_jaccard(ans, pred) >= IOU_THRESH or ans in pred\n                    else:\n                        is_match = get_jaccard(ans, pred) >= IOU_THRESH\n                    if is_match:\n                        match_found = True\n\n                if not match_found:\n                    fp += 1\n\n    precision = tp / (tp + fp) if tp + fp > 0 else np.nan\n    recall = tp / (tp + fn) if tp + fn > 0 else np.nan\n\n    return precision, recall\n\n\ndef process_precisions(precision):\n    \"\"\"\n    Processes precisions to ensure that precision and recall don't both get worse\n    Assumes the list precision is sorted in order of recalls\n    \"\"\"\n    precision_best = precision[::-1]\n    for i in range(1, len(precision_best)):\n        precision_best[i] = max(precision_best[i-1], precision_best[i])\n    precision = precision_best[::-1]\n    return precision\n\n\ndef get_prec_at_recall(precisions, recalls, confs, recall_thresh=0.9):\n    \"\"\"\n    Assumes recalls are sorted in increasing order\n    \"\"\"\n    processed_precisions = process_precisions(precisions)\n    prec_at_recall = 0\n    for prec, recall, conf in zip(processed_precisions, recalls, confs):\n        if recall >= recall_thresh:\n            prec_at_recall = prec\n            break\n    return prec_at_recall, conf\n\n\ndef get_precisions_recalls(pred_dict, gt_dict, category=None):\n    precisions = [1]\n    recalls = [0]\n    confs = []\n    for conf in list(np.arange(0.99, 0, -0.01)) + [0.001, 0]:\n        conf_thresh_pred_dict = get_preds(pred_dict, conf)\n        prec, recall = compute_precision_recall(gt_dict, conf_thresh_pred_dict, category=category)\n        precisions.append(prec)\n        recalls.append(recall)\n        confs.append(conf)\n    return precisions, recalls, confs\n\n\ndef get_aupr(precisions, recalls):\n    processed_precisions = process_precisions(precisions)\n    aupr = metrics.auc(recalls, processed_precisions)\n    if np.isnan(aupr):\n        return 0\n    return aupr\n\n\ndef get_results(model_path, gt_dict, verbose=False):\n    predictions_path = os.path.join(model_path, \"nbest_predictions_.json\")\n    name = model_path.split(\"/\")[-1]\n\n    pred_dict = load_json(predictions_path)\n    print(len(list(gt_dict.keys())))\n    assert sorted(list(pred_dict.keys())) == sorted(list(gt_dict.keys()))\n\n    precisions, recalls, confs = get_precisions_recalls(pred_dict, gt_dict)\n    prec_at_90_recall, _ = get_prec_at_recall(precisions, recalls, confs, recall_thresh=0.9)\n    prec_at_80_recall, _ = get_prec_at_recall(precisions, recalls, confs, recall_thresh=0.8)\n    aupr = get_aupr(precisions, recalls)\n\n    \n    print(\"AUPR: {:.3f}, Precision at 80% Recall: {:.3f}, Precision at 90% Recall: {:.3f}\".format(aupr, prec_at_80_recall, prec_at_90_recall))\n\n    # now save results as a dataframe and return\n    results = {\"name\": name, \"aupr\": aupr, \"prec_at_80_recall\": prec_at_80_recall, \"prec_at_90_recall\": prec_at_90_recall}\n    return results\n\n\n# if name == \"main\":\n#     test_json_path = \"./data/test.json\"\n#     model_path = \"./trained_models/roberta-base\"\n#     save_dir = \"./results\"\n#     if not os.path.exists(save_dir): os.mkdir(save_dir)\n\n#     gt_dict = load_json(test_json_path)\n#     gt_dict = get_answers(gt_dict)\n\n#     results = get_results(model_path, gt_dict, verbose=True)\n\n#     save_path = os.path.join(save_dir, \"{}.json\".format(model_path.split(\"/\")[-1]))\n#     with open(save_path, \"w\") as f:\n#         f.write(\"{}\\n\".format(results))","metadata":{"execution":{"iopub.status.busy":"2022-12-01T02:13:25.325602Z","iopub.execute_input":"2022-12-01T02:13:25.326253Z","iopub.status.idle":"2022-12-01T02:13:25.740882Z","shell.execute_reply.started":"2022-12-01T02:13:25.326206Z","shell.execute_reply":"2022-12-01T02:13:25.739797Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"gt_dict = load_json(\"/kaggle/working/cuad-main/data/test_final.json\")\ngt_dict = get_answers(gt_dict)\nlen(gt_dict.keys())\n\nresults = get_results(\"./train_models/albert-base-v2/\", gt_dict, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T02:13:25.742427Z","iopub.execute_input":"2022-12-01T02:13:25.742805Z","iopub.status.idle":"2022-12-01T02:13:25.883493Z","shell.execute_reply.started":"2022-12-01T02:13:25.742766Z","shell.execute_reply":"2022-12-01T02:13:25.882369Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"123\nAUPR: 0.103, Precision at 80% Recall: 0.000, Precision at 90% Recall: 0.000\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}