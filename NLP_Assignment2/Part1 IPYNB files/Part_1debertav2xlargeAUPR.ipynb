{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3acf2e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "IOU_THRESH = 0.5\n",
    "\n",
    "def get_questions_from_csv():\n",
    "    df = pd.read_csv(\"category_descriptions.csv\")\n",
    "    q_dict = {}\n",
    "    for i in range(df.shape[0]):\n",
    "        category = df.iloc[i, 0].split(\"Category: \")[1]\n",
    "        description = df.iloc[i, 1].split(\"Description: \")[1]\n",
    "        q_dict[category.title()] = description\n",
    "    return q_dict\n",
    "\n",
    "qtype_dict = get_questions_from_csv()\n",
    "\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        dict = json.load(f)\n",
    "    return dict\n",
    "\n",
    "\n",
    "def get_preds(nbest_preds_dict, conf=None):\n",
    "    results = {}\n",
    "    for question_id in nbest_preds_dict:\n",
    "        list_of_pred_dicts = nbest_preds_dict[question_id]\n",
    "        preds = {}\n",
    "        for pred_dict in list_of_pred_dicts:\n",
    "            text = pred_dict[\"text\"]\n",
    "            prob = pred_dict[\"probability\"]\n",
    "            if not text == \"\":  # don't count empty string as a prediction\n",
    "                preds[text] = prob\n",
    "        preds_list = [pred for pred in preds.keys() if preds[pred] > conf]\n",
    "        results[question_id] = preds_list\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_answers(test_json_dict):\n",
    "    results = {}\n",
    "\n",
    "    data = test_json_dict[\"data\"]\n",
    "    for contract in data:\n",
    "        for para in contract[\"paragraphs\"]:\n",
    "            qas = para[\"qas\"]\n",
    "            for qa in qas:\n",
    "                id = qa[\"id\"]\n",
    "                answers = qa[\"answers\"]\n",
    "                answers = [answers[i][\"text\"] for i in range(len(answers))]\n",
    "                results[id] = answers\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_jaccard(gt, pred):\n",
    "    remove_tokens = [\".\", \",\", \";\", \":\"]\n",
    "    for token in remove_tokens:\n",
    "        gt = gt.replace(token, \"\")\n",
    "        pred = pred.replace(token, \"\")\n",
    "    gt = gt.lower()\n",
    "    pred = pred.lower()\n",
    "    gt = gt.replace(\"/\", \" \")\n",
    "    pred = pred.replace(\"/\", \" \")\n",
    "\n",
    "    gt_words = set(gt.split(\" \"))\n",
    "    pred_words = set(pred.split(\" \"))\n",
    "\n",
    "    intersection = gt_words.intersection(pred_words)\n",
    "    union = gt_words.union(pred_words)\n",
    "    jaccard = len(intersection) / len(union)\n",
    "    return jaccard\n",
    "\n",
    "\n",
    "def compute_precision_recall(gt_dict, preds_dict, category=None):\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "\n",
    "    for key in gt_dict:\n",
    "        if category and category not in key:\n",
    "            continue\n",
    "\n",
    "        substr_ok = \"Parties\" in key\n",
    "\n",
    "        answers = gt_dict[key]\n",
    "        preds = preds_dict[key]\n",
    "\n",
    "        # first check if answers is empty\n",
    "        if len(answers) == 0:\n",
    "            if len(preds) > 0:\n",
    "                fp += len(preds)  # false positive for each one\n",
    "        else:\n",
    "            for ans in answers:\n",
    "                assert len(ans) > 0\n",
    "                # check if there is a match\n",
    "                match_found = False\n",
    "                for pred in preds:\n",
    "                    if substr_ok:\n",
    "                        is_match = get_jaccard(ans, pred) >= IOU_THRESH or ans in pred\n",
    "                    else:\n",
    "                        is_match = get_jaccard(ans, pred) >= IOU_THRESH\n",
    "                    if is_match:\n",
    "                        match_found = True\n",
    "\n",
    "                if match_found:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fn += 1\n",
    "\n",
    "            # now also get any fps by looping through preds\n",
    "            for pred in preds:\n",
    "                # Check if there's a match. if so, don't count (don't want to double count based on the above)\n",
    "                # but if there's no match, then this is a false positive.\n",
    "                # (Note: we get the true positives in the above loop instead of this loop so that we don't double count\n",
    "                # multiple predictions that are matched with the same answer.)\n",
    "                match_found = False\n",
    "                for ans in answers:\n",
    "                    assert len(ans) > 0\n",
    "                    if substr_ok:\n",
    "                        is_match = get_jaccard(ans, pred) >= IOU_THRESH or ans in pred\n",
    "                    else:\n",
    "                        is_match = get_jaccard(ans, pred) >= IOU_THRESH\n",
    "                    if is_match:\n",
    "                        match_found = True\n",
    "\n",
    "                if not match_found:\n",
    "                    fp += 1\n",
    "\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else np.nan\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else np.nan\n",
    "\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def process_precisions(precision):\n",
    "    \"\"\"\n",
    "    Processes precisions to ensure that precision and recall don't both get worse\n",
    "    Assumes the list precision is sorted in order of recalls\n",
    "    \"\"\"\n",
    "    precision_best = precision[::-1]\n",
    "    for i in range(1, len(precision_best)):\n",
    "        precision_best[i] = max(precision_best[i-1], precision_best[i])\n",
    "    precision = precision_best[::-1]\n",
    "    return precision\n",
    "\n",
    "\n",
    "def get_prec_at_recall(precisions, recalls, confs, recall_thresh=0.9):\n",
    "    \"\"\"\n",
    "    Assumes recalls are sorted in increasing order\n",
    "    \"\"\"\n",
    "    processed_precisions = process_precisions(precisions)\n",
    "    prec_at_recall = 0\n",
    "    for prec, recall, conf in zip(processed_precisions, recalls, confs):\n",
    "        if recall >= recall_thresh:\n",
    "            prec_at_recall = prec\n",
    "            break\n",
    "    return prec_at_recall, conf\n",
    "\n",
    "\n",
    "def get_precisions_recalls(pred_dict, gt_dict, category=None):\n",
    "    precisions = [1]\n",
    "    recalls = [0]\n",
    "    confs = []\n",
    "    for conf in list(np.arange(0.99, 0, -0.01)) + [0.001, 0]:\n",
    "        conf_thresh_pred_dict = get_preds(pred_dict, conf)\n",
    "        prec, recall = compute_precision_recall(gt_dict, conf_thresh_pred_dict, category=category)\n",
    "        precisions.append(prec)\n",
    "        recalls.append(recall)\n",
    "        confs.append(conf)\n",
    "    return precisions, recalls, confs\n",
    "\n",
    "\n",
    "def get_aupr(precisions, recalls):\n",
    "    processed_precisions = process_precisions(precisions)\n",
    "    aupr = metrics.auc(recalls, processed_precisions)\n",
    "    if np.isnan(aupr):\n",
    "        return 0\n",
    "    return aupr\n",
    "\n",
    "\n",
    "def get_results(model_path, gt_dict, verbose=False):\n",
    "    predictions_path = os.path.join(model_path, \"nbest_predictions_.json\")\n",
    "    name = model_path.split(\"/\")[-1]\n",
    "\n",
    "    pred_dict = load_json(predictions_path)\n",
    "\n",
    "    assert sorted(list(pred_dict.keys())) == sorted(list(gt_dict.keys()))\n",
    "\n",
    "    precisions, recalls, confs = get_precisions_recalls(pred_dict, gt_dict)\n",
    "    prec_at_90_recall, _ = get_prec_at_recall(precisions, recalls, confs, recall_thresh=0.9)\n",
    "    prec_at_80_recall, _ = get_prec_at_recall(precisions, recalls, confs, recall_thresh=0.8)\n",
    "    aupr = get_aupr(precisions, recalls)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"AUPR: {:.3f}, Precision at 80% Recall: {:.3f}, Precision at 90% Recall: {:.3f}\".format(aupr, prec_at_80_recall, prec_at_90_recall))\n",
    "\n",
    "    # now save results as a dataframe and return\n",
    "    results = {\"name\": name, \"aupr\": aupr, \"prec_at_80_recall\": prec_at_80_recall, \"prec_at_90_recall\": prec_at_90_recall}\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ce8f0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUPR: 0.478, Precision at 80% Recall: 0.440, Precision at 90% Recall: 0.178\n"
     ]
    }
   ],
   "source": [
    "gt_dict = load_json(\"./data/test.json\")7\n",
    "gt_dict = get_answers(gt_dict)\n",
    "len(gt_dict.keys())\n",
    "results = get_results(\"./deberta-v2-xlarge/deberta-v2-xlarge/\", gt_dict, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8194c203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
