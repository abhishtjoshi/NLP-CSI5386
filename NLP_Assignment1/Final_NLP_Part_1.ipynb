{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yni-7IbrPp5d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eGnfLuEaUXZD",
    "outputId": "37911561-9d92-4e2e-ada2-c517cb4a13c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aIk8X1y9UY73"
   },
   "outputs": [],
   "source": [
    "!unzip /content/drive/MyDrive/CUAD_v1.zip\n",
    "#UNCOMMENT IT LATER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gLLlXCIZVD1A",
    "outputId": "04f988b8-5109-4ad8-f05a-46240f465192"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/CUAD_v1/full_contract_txt\n"
     ]
    }
   ],
   "source": [
    "%cd /content/CUAD_v1/full_contract_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WR_icGUsLNbK"
   },
   "source": [
    "## **Corpus Formation by concatenating 510 text files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwoYvCrFVTJ_"
   },
   "outputs": [],
   "source": [
    "# Concatenating file code should be run once due to its huge processing time.\n",
    "\n",
    "import fileinput\n",
    "import glob\n",
    "sentenceLines = []\n",
    "file_list = glob.glob(\"*.txt\")\n",
    "with open('result.txt', 'w') as file:\n",
    "    input_lines = fileinput.input(file_list)\n",
    "    file.writelines(input_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Egs6SPWeX_Zo"
   },
   "outputs": [],
   "source": [
    "f = open(\"/content/CUAD_v1/full_contract_txt/result.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WZizH_msjzCh",
    "outputId": "87d0bc12-91a9-4765-b7e4-b61f7dba51cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26807133\n"
     ]
    }
   ],
   "source": [
    "data=f.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H8a53-nOkOxh",
    "outputId": "10608145-4bda-4272-81d7-1953968c365d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2tqhuglkTLv"
   },
   "source": [
    "### **Tokenization of corpus without data cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vvCxtJu-kVUL"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "tokens = word_tokenize(data) # tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IidqUyAKs4YT"
   },
   "outputs": [],
   "source": [
    "#Printing the tokenized output of the whole corpus in the Output.txt file\n",
    "with open(\"Output.txt\", 'w') as f:\n",
    "    for k in  tokens:\n",
    "        f.write(\"{}\\n\".format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oLkPVG0LkaCB",
    "outputId": "a291a7d6-92d7-4e17-f681-181e91ba6bde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting no of tokens after tokenization:   4775316\n"
     ]
    }
   ],
   "source": [
    "#Counting number of tokens after tokenization\n",
    "print(\"Counting no of tokens after tokenization:  \", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yiT-xRbWkle3"
   },
   "outputs": [],
   "source": [
    "#Counting token frequency\n",
    "from collections import Counter\n",
    "tokencounts = Counter(tokens)\n",
    "tokencounts.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cv1nSoRQknI-",
    "outputId": "de6f5078-280c-416d-d3ca-0c2b6d1799e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of tokens which appear once:   20423\n",
      "Count of unique tokens:  47070\n"
     ]
    }
   ],
   "source": [
    "#Tokens which occur only once.\n",
    "max_occurance = 1\n",
    "singlecount_tokens = [k for k,v in tokencounts.items() if v == max_occurance]\n",
    "print(\"Count of tokens which appear once:  \", len(singlecount_tokens))\n",
    "\n",
    "#Unique count of tokens\n",
    "print(\"Count of unique tokens: \",len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zZD8WDggsKmP",
    "outputId": "e403196f-372e-4f4b-8d1d-a706f405ce40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009856939310403752\n"
     ]
    }
   ],
   "source": [
    "#Type/token Ratio\n",
    "Type_Token_Ratio= len(set(tokens))/len(tokens)\n",
    "print(Type_Token_Ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjVAN30VkpNB"
   },
   "outputs": [],
   "source": [
    "#For each token, print the token and its frequency [max to least] in a file called tokens.txt\n",
    "with open(\"tokens.txt\", 'w') as f:\n",
    "    for k,v in  tokencounts.most_common():\n",
    "        f.write( \"{} {}\\n\".format(k,v) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1J-Ou-CDkvhe"
   },
   "source": [
    "## **Tokenization after Punctuation removal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qdw5A0f3kxqf"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "datawithoutpunctuation= re.sub(r'[^a-zA-Z\\s]', ' ', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAJS4i8Yk8Jb"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "PRem_tokens = word_tokenize(datawithoutpunctuation) # tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4fkZTfOAlAKw",
    "outputId": "89c3b06b-d8c3-401c-ef7b-076cae2a7e10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting no of tokens after tokenization and punctuations and symbols removal:   3921915\n"
     ]
    }
   ],
   "source": [
    "#counting no of tokens after tokenization and removing punctuation\n",
    "print(\"Counting no of tokens after tokenization and punctuations and symbols removal:  \",len(PRem_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TbnnzvoclCDk"
   },
   "outputs": [],
   "source": [
    "#Counting token frequency - top 20\n",
    "from collections import Counter\n",
    "PRem_tokencounts = Counter(PRem_tokens)\n",
    "print(*PRem_tokencounts.most_common(20), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I7qJV0Pwt8Av",
    "outputId": "780c48a2-322a-41ce-bac1-65e5b8ab6d61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006771946867792902\n"
     ]
    }
   ],
   "source": [
    "#Type/token Ratio, Lexical Diversity\n",
    "Prem_Type_Token_Ratio= len(set(PRem_tokens))/len(PRem_tokens)\n",
    "print(Prem_Type_Token_Ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "an7R-yHSlUkv"
   },
   "source": [
    "## **Tokenization after StopWords removal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v0rZ-8solWS9"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "tokens = word_tokenize(datawithoutpunctuation) # tokenization\n",
    "#new_tokens = [t.lower() for t in new_tokens]\n",
    "stopwordrem_tokens =[t for t in tokens if t not in stopwords.words('english')] #token after stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4izOdabPlarV",
    "outputId": "418cb26f-e891-4433-e373-06cf1916e830"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting no of tokens after tokenization and stopwords:   2218468\n",
      "26423\n"
     ]
    }
   ],
   "source": [
    "#counting no of tokens after tokenization and removing stopwords\n",
    "print(\"Counting no of tokens after tokenization and stopwords:  \",len(stopwordrem_tokens))\n",
    "\n",
    "#unique count\n",
    "print(len(set(stopwordrem_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vLV1LT2lfHr"
   },
   "outputs": [],
   "source": [
    "#Counting token frequency\n",
    "from collections import Counter\n",
    "stopwordrem_tokencounts = Counter(stopwordrem_tokens)\n",
    "stopwordrem_tokencounts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DhsGnO_0vFF2",
    "outputId": "643cc679-56d2-4a07-8da0-ff20d9483455"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011910471550637647\n"
     ]
    }
   ],
   "source": [
    "#Type/token Ratio, Lexical Density\n",
    "stopwordrem_Type_Token_Ratio= len(set(stopwordrem_tokens))/len(stopwordrem_tokens)\n",
    "print(stopwordrem_Type_Token_Ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdZtBxTblm67"
   },
   "source": [
    "## **Bigram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3c6uoKQ_loNn"
   },
   "outputs": [],
   "source": [
    "def addToExisingDictionary(dictt, valuelist, line):\n",
    "  for item in valuelist:\n",
    "    if item[0] not in dictt:\n",
    "      dictt[item[0]] = item[1]\n",
    "    else:\n",
    "      dictt[item[0]]+=item[1]\n",
    "  return dictt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNFMv1ipfDsa"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from itertools import islice\n",
    "from nltk import bigrams\n",
    "modifiedData = data+\".\"\n",
    "tokenizedSentences = sent_tokenize(modifiedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ALYdUPFl8Il",
    "outputId": "92693ae5-9d23-4397-8bb5-3ff19fac7a58"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final: 13\n",
      "656939\n",
      "[(('set', 'forth'), 6152), (('third', 'party'), 4607), (('agreement', 'shall'), 3659), (('confidential', 'information'), 3601), (('party', 'shall'), 3283), (('intellectual', 'property'), 2936), (('effective', 'date'), 2823), (('either', 'party'), 2448), (('written', 'notice'), 2385), (('terms', 'conditions'), 2087)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import *\n",
    "nltk.download('stopwords')\n",
    "finalBigrams = dict()\n",
    "for line in tokenizedSentences:\n",
    "  a=[]\n",
    "  words = re.sub(r'[^a-zA-Z\\s]', ' ', line)\n",
    "  tokenizedWords = word_tokenize(words)\n",
    "  filteredData =[ token for token in tokenizedWords if token not in stopwords.words('english')]\n",
    "  finder = BigramCollocationFinder.from_words(filteredData)\n",
    "  a=list(finder.ngram_fd.items())\n",
    "  a.sort(key=lambda item: item[-1], reverse=True)\n",
    "  finalBigrams = addToExisingDictionary(finalBigrams, list(a), line)\n",
    "finalBigrams = dict(sorted(finalBigrams.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "#all pairs are stored in finalBigrams\n",
    "#this print statement prints out the top 20.\n",
    "print(list(islice(finalBigrams.items(), 20)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
